\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% \usepackage{ngerman}  % german documents
\usepackage{graphicx}  % import graphics einbinden
\usepackage{listings}  % support source code listing
\usepackage{amsmath}  % math stuff
\usepackage{amssymb} % 
\usepackage{a4wide} % wide pages
\usepackage{fancyhdr} % nice headers
\usepackage{tikz} %graphs
\lstset{basicstyle=\footnotesize,language=Python,numbers=left, numberstyle=\tiny, stepnumber=5,firstnumber=0, numbersep=5pt} % set up listings
\pagestyle{fancy}             % header
\setlength{\parindent}{0pt}   % no indentation

\usepackage[pdfpagemode=None, colorlinks=true,  % url coloring
           linkcolor=blue, urlcolor=blue, citecolor=blue, plainpages=false, 
           pdfpagelabels,unicode]{hyperref}
           
% change enums style: first level (a), (b), (c)           
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

%lecture name
\newcommand{\lecture}{
	Bioinformatics III
}           

%assignment iteration
\newcommand{\assignment}{
	Third Assignment
}

%set up names, matricle number, and email
\newcommand{\authors}{
  \begin{tabular}{rl}
    \href{mailto:s9alfloh@stud.uni-saarland.de}{Alexander Flohr} & (2549738)\\
    \href{mailto:s9ankupi@stud.uni-saarland.de}{Andrea Kupitz} & (2550260)
  \end{tabular}
}

% use to start a new exercise
\newcommand{\exercise}[1]
{
  \stepcounter{subsection}
  \subsection*{Exercise \thesubsection: #1}

}

\begin{document}
\title{\Large \lecture \\ \textbf{\normalsize \assignment}}
\author{\authors}

\setlength \headheight{25pt}
\fancyhead[R]{\begin{tabular}{r}\lecture \\ \assignment \end{tabular}}
\fancyhead[L]{\authors}


\setcounter{section}{3} % modify for later sheets, i.e. 2, 3, ...
%\section{Introduction to Python and some Network Properties} % optional, note that section invocation sets the section counter + 1, so adapt the setcounter command
\maketitle

\exercise{Naive Bayes classifier}
\begin{enumerate}
\item Because the features are independent: \begin{equation}P(S) = \prod_{i} P(S_i) \end{equation}
Following the Bayes theorem: \begin{equation} \frac{P(S|C) * P(C)} {P(S)} = P(C|S) \end{equation}
Further \begin{equation} log( \frac{P(C|S)} {P(\bar{C}|S)} ) = log( \prod_{i} \frac{ \frac{P(S_i|C) * P(C)}{P(S_i)} }{ \frac{P(S_i|\bar{C}) * P(\bar{C})}{P(S_i)} } ) = \sum_{i} log( \frac{P(S_i|C) * P(C)}{P(S_i|\bar{C}) * P(\bar{C})} ) \end{equation}
The actual classification is done by evaluating the sum over the log-likelihood ratios. If the sum is smaller zero, it means most of the log-likelihoods were smaller zero because the probability of $\bar{C}$ was greater than the one for C. Therefore, the state sequence evaluates to "no interaction". Vice versa, if the sum over the likelihoods was greater than zero, we classify 1, which means there is an interaction between the proteins.

\item The logarithm of the ratio enables to see directly if the probability is higher that the observed states belong to interacting or non-interacting proteins.
Moreover the ratio of the likelihoods normalizes the probabilities because states occurring more often will have the same ratio than less frequent ones.\\
This classification may perform bad on real-world data if a feature classifies all interactions to the same class because we cannot handle probabilities beeing zero. Moreover classification is bad if the dataset is small because the classification may be biased to one class.

\item Listing \ref{ex1-1} shows source code.
\lstinputlisting[label=ex1-1,caption={Example Listing of source code}] {model.py}
P(C) = 0.5125628140703518\\
P($\bar{C}$) = 0.48743718592964824\\
Table \ref{tab1} shows best 10 absolute features.\\
The 10 features in table \ref{tab1} seem to be the most promising features to recognize protein interactions correctly because their log-likelihood is very high. There is no specific region of features which seems to be very helpful. The best ten features are rather widely spread. Further, no state variant seems to indicate an interaction.
\begin{table}[b]
\label{tab1}
\begin{tabular}{llll}
enumeration of best features & log ratio & feature number & state variant\\
\hline
1 & -1.896088525279219 & 33 & 0\\
2 & 1.2025011337144798 & 25 & 2\\
3 & 1.0175787952204678 & 33 & 2\\
4 & 0.7074238669166282 & 33 & 1\\
5 & 0.642885345779057 & 40 & 2\\
6 & -0.6380484996830071 & 57 & 2\\
7 & 0.6003257313602611 & 81 & 0\\
8 & -0.5271859068711975 & 95 & 2\\
9 & -0.4885167657120435 & 88 & 0\\
10 & -0.4885167657120435 & 89 & 1\\
\end{tabular}
\caption{best 10 features}
\end{table}

\item Listing \ref{ex1-1} shows source code.\\
The classification is equally distributed between both classes. But the accuracy is rather low.\\
For "test1.tsv" the classification is: [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1]\\
Whereby each position in the list corresponds to the protein pair in the testfile at the same position. The exact value of the accuracy is 0.5757575757575758.

\item P(C) = 0.7839195979899497\\
P($\bar{C}$) = 0.2160804020100503\\
Table \ref{tab2} shows best 10 features.\\
\begin{table}[b]
\label{tab2}
\begin{tabular}{llll}
enumeration of best features & log ratio & feature number & state variant\\
\hline
1 & -1.9818030721159199 & 33 & 0\\
2 & 1.2762934659055623 & 83 & 0\\
3 & 0.9886113934537812 & 99 & 0\\
4 & 0.9085686857802447 & 98 & 3\\
5 & 0.8514102719402962 & 99 & 3\\
6 & -0.7290401036205519 & 87 & 1\\
7 & 0.7166776779701394 & 74 & 0\\
8 & 0.7037742731342315 & 90 & 1\\
9 & 0.6774569648168581 & 91 & 1\\
10 & 0.6155815610987706 & 63 & 0\\
\end{tabular}
\caption{best 10 features}
\end{table}
\end{enumerate}
For "test2.tsv" the classification is: [0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0]\\
The accuracy is 0.37373737373737376.\\
The classification might be worse for the second test set because in the second trainingset, interacting proteins occur more often than non-interacting ones which may lead to proteins rather been interpreted as interacting.

\exercise{Network communities}
\begin{enumerate}
\item The maximal finite value that the edge-clustering coefficiant can take is 2 because it is maximal if the numerator is maximal and the denominator is minimal. The denominator cannot get smaller than 0 because every pair of nodes will have at least one edge between them. Because division by zero is undefined, we take the next value 1, which means that the nodes both have at least one more edge than the one to each other. If both nodes have at least two edges, there is maximal one triangle between them if they are connected to the same node. Therefore, the numerator is 2. If there were more trinagles between the nodes, the denominator would also increase and the ratio is smaller. Thus, 2 is the maximal value the coefficient can take. Figure \ref{fig2_1} shows an example of a nettwork with maximal clustering coefficient between all nodes.\\
\newline
\begin{figure}
\begin{center}
\begin{tikzpicture}[auto, node distance=3cm, every loop/.style={},
                    thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
  \label{fig2_1}
  \node[main node] (1) {1};
  \node[main node] (2) [below left of=1] {2};
  \node[main node] (4) [below right of=1] {4};
  \path[every node/.style={font=\sffamily\small}]
    (1) edge node [left] {} (4)
    (2) edge node [right] {} (1)
        edge node {} (4);
\end{tikzpicture}
\caption{example of network with maximal clustering coefficient}
\end{center}
\end{figure}

\item Listing \ref{ex2-2} shows source code.
%\lstinputlisting[label=ex2-2,caption={Example Listing of source code}] {.py}
\begin{enumerate}
\item
\item
\end{enumerate}
\end{enumerate}

\end{document}